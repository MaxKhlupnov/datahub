# Default overrides for running local development.

# Images here are made as "development" images by following the general pattern of defining a multistage build with
# separate prod/dev steps; using APP_ENV to specify which to use. The dev steps should avoid building and instead assume
# that binaries and scripts will be mounted to the image, as also set up by this file. Also see see this excellent
# thread https://github.com/docker/cli/issues/1134.

# To make a JVM app debuggable via IntelliJ, go to its env file and add JVM debug flags, and then add the JVM debug
# port to this file.
---
version: '3.9'
services:
  datahub-frontend-react:
    container_name: khlupnov-datahub-frontend-react
    image: linkedin/datahub-frontend-react:v0.10.4-SNAPSHOT
    ports:
    - ${DATAHUB_MAPPED_FRONTEND_DEBUG_PORT:-5002}:5002
    - ${DATAHUB_MAPPED_FRONTEND_PORT:-9002}:9002
    
    depends_on:
      - datahub-gms
    environment:
    - JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5002
    - DATAHUB_ANALYTICS_ENABLED=${DATAHUB_ANALYTICS_ENABLED:-true}

    - DATAHUB_GMS_HOST=datahub-gms
    - DATAHUB_GMS_PORT=8090
    - DATAHUB_SECRET=YouKnowNothing
    - DATAHUB_APP_VERSION=1.0
    - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
    - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf
      -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml
      -Dlogback.debug=false -Dpidfile.path=/dev/null
    - KAFKA_BOOTSTRAP_SERVER=broker:29092
    - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
    - ELASTIC_CLIENT_HOST=elasticsearch
    - ELASTIC_CLIENT_PORT=9200

    volumes:
    - ../datahub-frontend/build/stage/playBinary:/datahub-frontend
  datahub-gms:
    container_name: khlupnov-datahub-gms
    image: linkedin/datahub-gms:v0.10.4-SNAPSHOT
    ports:
    - ${DATAHUB_MAPPED_GMS_DEBUG_PORT:-5001}:5001
    - 8090:8080
    depends_on:
      - postgres
      - elasticsearch
      - broker
    environment:
    - SKIP_ELASTICSEARCH_CHECK=false
   # - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-dev}
    - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}    
    - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
    - METADATA_SERVICE_AUTH_ENABLED=false
    - JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5001
    - BOOTSTRAP_SYSTEM_UPDATE_WAIT_FOR_SYSTEM_UPDATE=false
    - SEARCH_SERVICE_ENABLE_CACHE=false
    - LINEAGE_SEARCH_CACHE_ENABLED=false
    - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
    - EBEAN_DATASOURCE_USERNAME=datahub
    - EBEAN_DATASOURCE_PASSWORD=datahub
    - EBEAN_DATASOURCE_HOST=postgres:5432
    - EBEAN_DATASOURCE_URL=jdbc:postgresql://postgres:5432/datahub
    - EBEAN_DATASOURCE_DRIVER=org.postgresql.Driver
    # Uncomment EBEAN_POSTGRES_USE_AWS_IAM_AUTH below to add support for IAM authentication for Postgres.
    # Password is not required when accessing Postgres using IAM auth. It can be replaced by dummy password
    # EBEAN_POSTGRES_USE_AWS_IAM_AUTH=true
    - KAFKA_BOOTSTRAP_SERVER=broker:29092
    - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
    # KAFKA_SCHEMAREGISTRY_URL=http://datahub-gms:8080/schema-registry/api/
    - GRAPH_SERVICE_IMPL=elasticsearch
    - ELASTICSEARCH_HOST=elasticsearch
    - ELASTICSEARCH_PORT=9200
    - UI_INGESTION_ENABLED=true
    - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
    - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
    - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
    - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
    volumes:
    - ./datahub-gms/start.sh:/datahub/datahub-gms/scripts/start.sh
    - ./datahub-gms/jetty.xml:/datahub/datahub-gms/scripts/jetty.xml
    - ./monitoring/client-prometheus-config.yaml:/datahub/datahub-gms/scripts/prometheus-config.yaml
    - ../metadata-models/src/main/resources/:/datahub/datahub-gms/resources
    - ../metadata-service/war/build/libs/:/datahub/datahub-gms/bin
    - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  datahub-upgrade:
    image: acryldata/datahub-upgrade:v0.10.4-SNAPSHOT
    build:
      context: datahub-upgrade
      dockerfile: Dockerfile
      args:
        APP_ENV: dev
    environment:
    - SKIP_ELASTICSEARCH_CHECK=false
    - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-dev}
    - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
    volumes:
    - ../datahub-upgrade/build/libs/:/datahub/datahub-upgrade/bin/
    - ../metadata-models/src/main/resources/:/datahub/datahub-gms/resources
    - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  # Pre-creates the search indices using local mapping/settings.json
  postgres:
    container_name: postgres
    hostname: postgres
    image: postgres:12.3
    environment:
      - POSTGRES_USER=datahub
      - POSTGRES_PASSWORD=datahub
    ports:
      - '5432:5432'
    volumes:
      - .postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
  broker:
    container_name: khlupnov-datamap-broker
    depends_on:
    - zookeeper
    environment:
    - KAFKA_BROKER_ID=1
    - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
    - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
    - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
    - KAFKA_HEAP_OPTS=-Xms256m -Xmx256m
    - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
    hostname: broker
    image: confluentinc/cp-kafka:7.2.2
    ports:
    - 9092:9092
  zookeeper:
    container_name: khlupnov-datamap-zookeeper
    environment:
    - ZOOKEEPER_CLIENT_PORT=2181
    - ZOOKEEPER_TICK_TIME=2000
    hostname: zookeeper
    image: confluentinc/cp-zookeeper:7.2.2
    ports:
    - 2181:2181
    volumes:
    - zkdata:/var/lib/zookeeper
  schema-registry:
    container_name: khlupnov-datamap-schema-registry
    depends_on:
    - broker
    environment:
    - SCHEMA_REGISTRY_HOST_NAME=schemaregistry
    - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=PLAINTEXT
    - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092
    hostname: schema-registry
    image: confluentinc/cp-schema-registry:7.2.2
    ports:
    - 8081:8081  
  elasticsearch:
      container_name: khlupnov-datamap-elasticsearch
      environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms256m -Xmx256m -Dlog4j2.formatMsgNoLookups=true
      healthcheck:
        retries: 4
        start_period: 2m
        test:
        - CMD-SHELL
        - curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s'
          || exit 1
      hostname: elasticsearch
      image: elasticsearch:7.17.3
      mem_limit: 1g
      ports:
      - 9200:9200
      volumes:
      - esdata:/usr/share/elasticsearch/data 
  elasticsearch-setup:
      container_name: elasticsearch-setup
      hostname: elasticsearch-setup
      image: linkedin/datahub-elasticsearch-setup:v0.10.4-SNAPSHOT 
      depends_on:
      - elasticsearch
      environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_PROTOCOL=http
      
volumes:
  esdata:
    driver: local
    driver_opts:
     o: bind
     type: none
     device: ${HOME}/dh_data/esdata
  zkdata:
   driver: local
   driver_opts:
    o: bind
    type: none
    device: ${HOME}/dh_data/zkdata


